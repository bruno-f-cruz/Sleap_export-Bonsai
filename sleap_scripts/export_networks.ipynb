{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import sleap\n",
    "import json\n",
    "import numpy as np\n",
    "from sleap.nn.inference import (\n",
    "    CentroidCrop,\n",
    "    CentroidInferenceModel,\n",
    "    TopDownInferenceModel,\n",
    "    FindInstancePeaks,\n",
    "    TopDownMultiClassFindPeaks,\n",
    "    TopDownMultiClassInferenceModel,\n",
    "    SingleInstanceInferenceModel,\n",
    "    SingleInstanceInferenceLayer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def export_frozen_graph(model, preds, output_path):\n",
    "\n",
    "    tensors = {}\n",
    "\n",
    "    for key, val in preds.items():\n",
    "        dtype = str(val.dtype) if isinstance(val.dtype, np.dtype) else repr(val.dtype)\n",
    "        tensors[key] = {\n",
    "            \"type\": f\"{type(val).__name__}\",\n",
    "            \"shape\": f\"{val.shape}\",\n",
    "            \"dtype\": dtype,\n",
    "            \"device\": f\"{val.device if hasattr(val, 'device') else 'N/A'}\",\n",
    "        }\n",
    "\n",
    "    with output_path as d:\n",
    "        model.export_model(d.as_posix(), tensors=tensors)\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        with tf.compat.v2.io.gfile.GFile(f\"{d}/frozen_graph.pb\", \"rb\") as f:\n",
    "            graph_def = tf.compat.v1.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            tf.import_graph_def(graph_def)\n",
    "\n",
    "        with open(f\"{d}/info.json\") as json_file:\n",
    "            info = json.load(json_file)\n",
    "\n",
    "        for tensor_info in info[\"frozen_model_inputs\"] + info[\"frozen_model_outputs\"]:\n",
    "\n",
    "            saved_name = (\n",
    "                tensor_info.split(\"Tensor(\")[1].split(\", shape\")[0].replace('\"', \"\")\n",
    "            )\n",
    "            saved_shape = ast.literal_eval(\n",
    "                tensor_info.split(\"shape=\", 1)[1].split(\"), \")[0] + \")\"\n",
    "            )\n",
    "            saved_dtype = tensor_info.split(\"dtype=\")[1].split(\")\")[0]\n",
    "\n",
    "            loaded_shape = tuple(graph.get_tensor_by_name(f\"import/{saved_name}\").shape)\n",
    "            loaded_dtype = graph.get_tensor_by_name(f\"import/{saved_name}\").dtype.name\n",
    "\n",
    "            assert saved_shape == loaded_shape\n",
    "            assert saved_dtype == loaded_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmpyzx4k2vb\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmpyzx4k2vb\\assets\n"
     ]
    }
   ],
   "source": [
    "#Export full topdown network with ID\n",
    "\n",
    "runs_folder = r\"C:\\Users\\neurogears\\Desktop\\Sleap_demo\\models\"\n",
    "id_model = tf.keras.models.load_model(runs_folder + \"\\\\\" + \"221027_multiclass.topdown\" + \"\\\\best_model.h5\", compile = False)\n",
    "\n",
    "centroid_model_path = runs_folder + \"\\\\\" + r\"221027_104636.centroid.n=88\\best_model.h5\"\n",
    "centroid_model = tf.keras.models.load_model(centroid_model_path, compile = False)\n",
    "\n",
    "#Make sure you set the crop size to the expected input of the Id layer\n",
    "centroid = CentroidCrop(\n",
    "    keras_model=centroid_model, crop_size=96, input_scale = 0.5)\n",
    "\n",
    "instance_peaks = TopDownMultiClassFindPeaks(keras_model=id_model, return_class_vectors = True)\n",
    "model = TopDownMultiClassInferenceModel(centroid, instance_peaks)\n",
    "preds = model.predict(np.zeros((4, 1088, 1440, 1), dtype=\"uint8\"))\n",
    "\n",
    "export_frozen_graph(model, preds, Path(runs_folder) / \"BonsaiModels\" /  \"topdown_id\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002816A1BDCA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002816A1BDCA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmpdx3gf39e\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmpdx3gf39e\\assets\n"
     ]
    }
   ],
   "source": [
    "#Export full topdown network without ID\n",
    "\n",
    "runs_folder = r\"C:\\Users\\neurogears\\Desktop\\Sleap_demo\\models\"\n",
    "pose_model = tf.keras.models.load_model(runs_folder + \"\\\\\" + \"221027_111451.centered_instance.n=88\" + \"\\\\best_model.h5\", compile = False)\n",
    "\n",
    "centroid_model_path = runs_folder + \"\\\\\" + r\"221027_104636.centroid.n=88\\best_model.h5\"\n",
    "centroid_model = tf.keras.models.load_model(centroid_model_path, compile = False)\n",
    "\n",
    "#Make sure you set the crop size to the expected input of the Id layer\n",
    "centroid = CentroidCrop(\n",
    "    keras_model=centroid_model, crop_size=96, input_scale = 0.5)\n",
    "\n",
    "instance_peaks = FindInstancePeaks(keras_model=pose_model)\n",
    "model = TopDownInferenceModel(centroid, instance_peaks)\n",
    "preds = model.predict(np.zeros((4, 1088, 1440, 1), dtype=\"uint8\"))\n",
    "\n",
    "export_frozen_graph(model, preds, Path(runs_folder) / \"BonsaiModels\" /  \"topdown\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002834826B048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002834826B048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmp94q1ut5d\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmp94q1ut5d\\assets\n"
     ]
    }
   ],
   "source": [
    "## Export a centroid model\n",
    "\n",
    "runs_folder = r\"C:\\Users\\neurogears\\Desktop\\Sleap_demo\\models\"\n",
    "\n",
    "centroid_model_path = runs_folder + \"\\\\\" + r\"221027_104636.centroid.n=88\\best_model.h5\"\n",
    "centroid_model = tf.keras.models.load_model(centroid_model_path, compile = False)\n",
    "\n",
    "#Make sure you set the crop size to the expected input of the Id layer\n",
    "centroid = CentroidCrop(\n",
    "    keras_model=centroid_model, crop_size=96, input_scale = 0.5, return_crops=False)\n",
    "\n",
    "model = CentroidInferenceModel(centroid)\n",
    "\n",
    "preds = model.predict(np.zeros((4, 1088, 1440, 1), dtype=\"uint8\"))\n",
    "\n",
    "export_frozen_graph(model, preds, Path(runs_folder) / \"BonsaiModels\" /  \"centroid\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmp9fd6v9uj\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\NEUROG~1\\AppData\\Local\\Temp\\tmp9fd6v9uj\\assets\n"
     ]
    }
   ],
   "source": [
    "#Single instance model (using full picture for now...)\n",
    "\n",
    "runs_folder = r\"C:\\Users\\neurogears\\Desktop\\Sleap_demo\\models\"\n",
    "\n",
    "single_instance_model_path = runs_folder + \"\\\\\" + r\"221027_092218.single_instance.n=88\\best_model.h5\"\n",
    "single_instance_model = tf.keras.models.load_model(single_instance_model_path, compile = False)\n",
    "\n",
    "model = SingleInstanceInferenceModel(\n",
    "    SingleInstanceInferenceLayer(keras_model=single_instance_model)\n",
    ")\n",
    "\n",
    "preds = model.predict(np.zeros((4, 1088, 1440, 1), dtype=\"uint8\"))\n",
    "\n",
    "export_frozen_graph(model, preds, Path(runs_folder) / \"BonsaiModels\" /  \"SingleInstance\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized model (Not supported in the package yet....)\n",
    "from test_run import *\n",
    "import test_utils\n",
    "from pathlib import Path\n",
    "from sleap.nn.peak_finding import find_local_peaks, find_global_peaks, make_centered_bboxes, crop_bboxes\n",
    "from sleap.nn.data.utils import  describe_tensors\n",
    "\n",
    "id_model = tf.keras.models.load_model(runs_folder + \"\\\\\" + run_name + \"\\\\best_model.h5\", compile = False)\n",
    "\n",
    "#Define the full mode\n",
    "class TopDownInferenceModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        this_model,\n",
    "        td_output_stride,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.this_model = this_model\n",
    "        self.td_output_stride = td_output_stride\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def stage2(self, im):\n",
    "        \"\"\"Stage 2: Predict pose in each crop\"\"\"\n",
    "        # Preprocessing\n",
    "        X = tf.cast(im, tf.float32) / 255\n",
    "        X = tf.image.resize(X, [96, 96])\n",
    "\n",
    "        # Forward pass\n",
    "        cms, class_probs = self.this_model(X)\n",
    "\n",
    "        # Find keypoints in each crop\n",
    "        #     pts: (n_centroids, n_nodes, 2)\n",
    "        #     vals: (n_centroids, n_nodes)\n",
    "        pts, vals = find_global_peaks(cms, threshold=0.2, refinement=\"integral\")\n",
    "\n",
    "        # Adjust coordinates for output stride\n",
    "        pts = pts * self.td_output_stride\n",
    "\n",
    "        return {\"instance_peaks\": pts, \"instance_peak_vals\": vals, \"class_probabilities\": class_probs}\n",
    "\n",
    "    def call(self, imgs):\n",
    "        preds = self.stage2(imgs)\n",
    "        return preds\n",
    "\n",
    "td_output_stride = 4\n",
    "td_input_size = [96, 96] #Hardcoded in the network\n",
    "\n",
    "inference_model = TopDownInferenceModel(id_model,td_output_stride)\n",
    "\n",
    "preds = inference_model.predict(np.zeros((4, 96, 96, 1), dtype=\"uint8\"))\n",
    "describe_tensors(preds)\n",
    "\n",
    "inference_model.save(\n",
    "    r\"C:\\Users\\neurogears\\Desktop\\SLEAPNetwork_v1\\Bonsai\\customNetwork\" + \"\\\\\" + run_name,\n",
    "    save_format=\"tf\", save_traces=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('sleap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9540ce0ed9afc647fa54b86a4e2995b3fb7dfa50b295044d3f1e3f49757342c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
